{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHi/DtAB4JQSUF5nJwB+KV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabh139/conFusionRestaurant/blob/master/churn_sle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Churn Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "-IjSs8rcGSJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup and Imports**"
      ],
      "metadata": {
        "id": "JVD4l7fsHjCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install shap"
      ],
      "metadata": {
        "id": "lLR5zqPaHtjF",
        "outputId": "8aca75f7-8f03-4cde-c594-9191b48dd5e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import scipy\n",
        "import random\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, brier_score_loss, log_loss, accuracy_score, precision_recall_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "id": "ZMOsqZ_IHwNZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper Functions**"
      ],
      "metadata": {
        "id": "6MNPLp7bIOot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Statistical Analysis Functions**"
      ],
      "metadata": {
        "id": "wKm5QwhLIVn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_side_kurtosis(data, side=\"left\"):\n",
        "    \"\"\"Calculates the kurtosis of either the left or right side of a distribution.\"\"\"\n",
        "    median = np.median(data)\n",
        "    side_data = data[data <= median] if side == \"left\" else data[data > median]\n",
        "\n",
        "    unique_values = np.unique(side_data)\n",
        "    if len(unique_values) < 2:\n",
        "        return f\"Not enough unique values on {side} side (found {len(unique_values)})\"\n",
        "\n",
        "    if np.isinf(side_data).any() or np.isnan(side_data).any():\n",
        "        return f\"Infinite or NaN values found on {side} side\"\n",
        "\n",
        "    try:\n",
        "        kurt = stats.kurtosis(side_data)\n",
        "        return float(kurt)\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating {side}-side kurtosis: {str(e)}\"\n",
        "\n",
        "def bootstrap_std_error(data, num_bootstrap=1000):\n",
        "    \"\"\"Calculates the standard error of the mean using bootstrapping.\"\"\"\n",
        "    bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True))\n",
        "                              for _ in range(num_bootstrap)])\n",
        "    return np.std(bootstrap_means)\n",
        "\n",
        "def find_optimal_threshold(y_true, y_pred_proba):\n",
        "    \"\"\"Finds the optimal probability threshold for classification based on F1-score.\"\"\"\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
        "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
        "    return optimal_threshold"
      ],
      "metadata": {
        "id": "TrC23-3TIar_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing Functions**"
      ],
      "metadata": {
        "id": "8inzB96wIcj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_transform(data, data_type='train', **kwargs):\n",
        "    \"\"\"\n",
        "    Performs data transformation. Replace with your actual transformation logic.\n",
        "    \"\"\"\n",
        "    # Add your custom transformations here\n",
        "    return data\n",
        "\n",
        "def identify_column_types(df, target_variable):\n",
        "    \"\"\"Identifies numerical and categorical columns.\"\"\"\n",
        "    numerical_cols = []\n",
        "    categorical_cols = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        if column == target_variable:\n",
        "            continue\n",
        "        if df[column].dtype in ['int64', 'float64']:\n",
        "            numerical_cols.append(column)\n",
        "        else:\n",
        "            categorical_cols.append(column)\n",
        "\n",
        "    return numerical_cols, categorical_cols\n",
        "\n",
        "def encode_columns(data, numerical_cols, categorical_cols):\n",
        "    \"\"\"Encodes categorical columns and scales numerical columns.\"\"\"\n",
        "    for col in categorical_cols:\n",
        "        data[col] = data[col].astype(str)\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_cols),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "        ])\n",
        "    transformed_data = preprocessor.fit_transform(data)\n",
        "\n",
        "    feature_names = numerical_cols + list(\n",
        "        preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
        "\n",
        "    return transformed_data, feature_names, preprocessor\n",
        "\n",
        "def data_preprocessing(data, target_variable, is_train=True):\n",
        "    \"\"\"Preprocesses data, including encoding.\"\"\"\n",
        "    y_data = data[target_variable]\n",
        "    X_data = data.drop(columns=[target_variable])\n",
        "\n",
        "    numerical_cols, categorical_cols = identify_column_types(X_data, target_variable=None)\n",
        "\n",
        "    if is_train:\n",
        "        transformed_data, feature_names, preprocessor = encode_columns(\n",
        "            X_data, numerical_cols, categorical_cols)\n",
        "    else:\n",
        "        transformed_data = preprocessor.transform(X_data)\n",
        "        feature_names = numerical_cols + list(\n",
        "            preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols))\n",
        "\n",
        "    if scipy.sparse.issparse(transformed_data):\n",
        "        X_data = pd.DataFrame(transformed_data.toarray(), columns=feature_names)\n",
        "    else:\n",
        "        X_data = pd.DataFrame(transformed_data, columns=feature_names)\n",
        "\n",
        "    return (X_data, y_data, preprocessor) if is_train else (X_data, y_data)"
      ],
      "metadata": {
        "id": "Z1ihu9p3IiyX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Training and Evaluation Functions**"
      ],
      "metadata": {
        "id": "TtbhpP-nImep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train, model_params={}, cv_params={}):\n",
        "    \"\"\"Trains the model (XGBoost with optional cross-validation).\"\"\"\n",
        "    default_cv_params = {\n",
        "        \"scoring\": \"f1\",\n",
        "        \"cv\": 5,\n",
        "        \"verbose\": 1,\n",
        "        \"n_iter\": 10,\n",
        "        \"random_state\": 42,\n",
        "        \"error_score\": 'raise'\n",
        "    }\n",
        "    cv_params = {**default_cv_params, **cv_params}\n",
        "\n",
        "    if cv_params[\"cv\"] >= 2:\n",
        "        model = RandomizedSearchCV(\n",
        "            XGBClassifier(**model_params),\n",
        "            cv_params\n",
        "        ).fit(X_train, y_train)\n",
        "        best_model = model.best_estimator_\n",
        "    else:\n",
        "        model = XGBClassifier(**model_params).fit(X_train, y_train)\n",
        "        best_model = model\n",
        "    return best_model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluates the trained model.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
        "    return metrics\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
        "    \"\"\"Calculates various evaluation metrics.\"\"\"\n",
        "    metrics = {\n",
        "        'Skewness': stats.skew(y_pred),\n",
        "        'Left-Side Kurtosis': calculate_side_kurtosis(y_pred_proba, \"left\"),\n",
        "        'Right-Side Kurtosis': calculate_side_kurtosis(y_pred_proba, \"right\"),\n",
        "        'Standard Error (Direct)': np.std(y_pred) / np.sqrt(len(y_pred)),\n",
        "        'Standard Error (Bootstrapped)': bootstrap_std_error(y_pred),\n",
        "        'Log Loss': log_loss(y_true, y_pred_proba),\n",
        "        'F1 Score': f1_score(y_true, y_pred),\n",
        "        'ROC-AUC': roc_auc_score(y_true, y_pred_proba),\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Optimal Threshold': find_optimal_threshold(y_true, y_pred_proba)\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "BaItKtxsIt8n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading and Initial Transformation**"
      ],
      "metadata": {
        "id": "cUIldq9BIvsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "# train_data = pd.read_csv(\"your_train_data.csv\")  # Replace with your training data path\n",
        "# eval_data = pd.read_csv(\"your_eval_data.csv\")    # Replace with your evaluation data path\n",
        "target_variable = \"your_target_variable\"          # Replace with your target column name\n",
        "\n",
        "# Apply transformations\n",
        "transformed_train_data = data_transform(train_data.copy(), data_type='train')\n",
        "transformed_eval_data = data_transform(eval_data.copy(), data_type='exec')"
      ],
      "metadata": {
        "id": "sGrWje70JYpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing and Train-Test Split**"
      ],
      "metadata": {
        "id": "HamVg_rAI63V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess training data\n",
        "X_train, y_train, preprocessor = data_preprocessing(\n",
        "    transformed_train_data, target_variable, is_train=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7Fuv0htQJYYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "wkIS-DZsJPEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "xgb_params = {\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"n_estimators\": 100,\n",
        "    \"max_depth\": 3,\n",
        "    \"tree_method\": 'hist',\n",
        "    \"enable_categorical\": True\n",
        "}\n",
        "\n",
        "# Define cross-validation parameters\n",
        "cv_params = {\"cv\": 5}\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(X_train, y_train, model_params=xgb_params, cv_params=cv_params)"
      ],
      "metadata": {
        "id": "jGnr77yBJYHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation on Test Set**"
      ],
      "metadata": {
        "id": "F7BirpxKJSvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_metrics = evaluate_model(trained_model, X_test, y_test)\n",
        "print(\"Test set metrics:\")\n",
        "for metric_name, metric_value in test_metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "nA9vWH99JXyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Real-world Evaluation**"
      ],
      "metadata": {
        "id": "rrpQb9NGJVw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess evaluation data\n",
        "X_eval, y_eval = data_preprocessing(transformed_eval_data, target_variable, is_train=False)\n",
        "\n",
        "# Handle column mismatches\n",
        "train_cols = X_train.columns\n",
        "missing_cols_eval = set(train_cols) - set(X_eval.columns)\n",
        "extra_cols_eval = set(X_eval.columns) - set(train_cols)\n",
        "\n",
        "for col in missing_cols_eval:\n",
        "    X_eval[col] = 0\n",
        "\n",
        "X_eval = X_eval.drop(columns=list(extra_cols_eval)) if extra_cols_eval else X_eval\n",
        "X_eval = X_eval[train_cols]\n",
        "\n",
        "# Evaluate on real-world data\n",
        "real_world_metrics = evaluate_model(trained_model, X_eval, y_eval)\n",
        "print(\"\\nReal-world evaluation metrics:\")\n",
        "for metric_name, metric_value in real_world_metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "id": "dPVUBovnJW-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}